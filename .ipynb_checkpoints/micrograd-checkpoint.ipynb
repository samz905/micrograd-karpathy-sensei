{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc7e74a-bdd4-4606-89d4-974576d4137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    # _var is a variable intended for internal use within a class\n",
    "    # self._var = var makes it publicly accessible as obj.var instead of obj._var\n",
    "    def __init__(self, data, _children=(), _op='', _exp='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None # Function to calculate local grads of the input nodes to this output node\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.exp = _exp\n",
    "        \n",
    "        \n",
    "    # The __repr__ method provides a string representation of the instance, which is useful for debugging and displaying the object    \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    \n",
    "    # The __add__ method is a special method used to define the behavior of the addition operator (+) for instances of a class\n",
    "    # Internally, the expression 'a + b' calls a.__add__(b)\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other) # So that we can add a numeric value directly to a Value object like Value(3.0) + 4\n",
    "        out = Value(self.data + other.data, (self, other), '+', f\"{self} + {other}\")\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad # += because if b = a + a, the db/da should be 2 but self first becomes 1 and then other (which is also a) becomes 1 so we want to accumulate instead of overwrite. Also if z = x + y and w = x * y, backprop must add dz/dx and dw/dx for x and so for y\n",
    "            other.grad += out.grad\n",
    "            \n",
    "        out._backward = _backward # Not out._backward = _backward() as lambda functions return None and also the object's _backward attribute has a function value so passing _backward() will pass its returned value, not the function itself. We simply set the function to out._backward and not it's executed value\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    # The __sub__ method is a special method used to define the behavior of the subtraction operator (-) for instances of a class\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    \n",
    "    def __rsub__(self, other): # other - self\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        return other + (-self)\n",
    "    \n",
    "    \n",
    "    # The __mul__ method is a special method used to define the behavior of the multiplication operator (*) for instances of a class\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*', f\"{self} * {other}\")\n",
    "        \n",
    "        # Multiply by out.grad because we are applying the chain rule from the output back to that local layer or expression\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad \n",
    "            other.grad += self.data * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    # Something like 2 * Value(3.0) will throw an error as we have defined self * other where self is the Value object. __rmul__ will swap the expression so that 2 * Value(3.0) will become Value(3.0) * 2 and now this goes to __mul__\n",
    "    def __rmul__(self, other):\n",
    "        return self * other \n",
    "    \n",
    "    \n",
    "    # The __truediv__ method is a special method used to define the behavior of the division operator (/) for instances of a class\n",
    "    def __truediv__(self, other):\n",
    "        return Value(self * other**-1, (self, other), '/', f\"{self} / {other}\") # We expressed / as a * equation so that the definition of * handles backprop without needing to redefine it\n",
    "    \n",
    "    \n",
    "    # The __neg__ method is a special method used to define the behavior of the negative operator (-) for instances of a class\n",
    "    def __neg__(self):\n",
    "        out = Value(self.data * -1, (self,), '-', f\"-{self}\")\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += -1 * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    # The __pow__ method is a special method used to define the behavior of the power operator (**) for instances of a class\n",
    "    def __pow__(self, other):\n",
    "        # assert isinstance(other, (int, float))\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data ** other.data, (self, other), '**', f\"{self} ^ {other}\")\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other.data * self.data**(other.data - 1) * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def tanh(self): # Can be called as x = Value(3, label='x'); x.tanh()\n",
    "        x = self.data\n",
    "        tanh = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "        out = Value(tanh, (self, ), label='tanh')\n",
    "        \n",
    "        def _backward():  \n",
    "            self.grad += (1 - tanh**2) * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self, ), 'exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def backward(self):\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        \n",
    "        # This function is expected to be called on the output node to initiate backprop from there so o.backward() will initiate backprop. Remember that we are finding grads for each weight as the derivative of the output w.r.t to that weight and for the output itself, do/do = 1\n",
    "        self.grad = 1.0\n",
    "        \n",
    "        for node in reversed(topo): # Reversed because the list is ordered from input layer to output layer and we wanna go in the backwards direction starting from the output for backprop\n",
    "            node._backward()\n",
    "            \n",
    "print(\"--------DONE--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee04213-1fd5-4177-953b-bc7688ba2844",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(3, label='a')\n",
    "b = Value(4, label='b')\n",
    "d = a + b; d.label='d'\n",
    "e = a - 3\n",
    "f = -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa52b239-59c1-4be4-a815-27258366f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e14ca3-8998-4ef1-9e5c-a873b3143350",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b7cc91-182d-4da2-a7fe-07f5c9086d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d._prev, d.exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34714777-b5a6-4a10-8cc2-a44828bd5b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6569ea-96bb-4277-a89a-caed3d3ba10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from graphviz import Digraph\n",
    "\n",
    "# def trace(root):\n",
    "#     nodes, edges = set(), set()\n",
    "#     def build(v):\n",
    "#         if v not in nodes:\n",
    "#             nodes.add(v)\n",
    "#             for child in v._prev:\n",
    "#                 edges.add((child, v))\n",
    "#                 build(child)\n",
    "#     build(root)\n",
    "#     return nodes, edges\n",
    "\n",
    "# def draw_dot(root, format='svg', rankdir='LR'):\n",
    "#     \"\"\"\n",
    "#     format: png | svg | ...\n",
    "#     rankdir: TB (top to bottom graph) | LR (left to right)\n",
    "#     \"\"\"\n",
    "#     assert rankdir in ['LR', 'TB']\n",
    "#     nodes, edges = trace(root)\n",
    "#     dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n",
    "    \n",
    "#     for n in nodes:\n",
    "#         dot.node(name=str(id(n)), label = \"{ data %.4f | grad %.4f }\" % (n.data, n.grad), shape='record')\n",
    "#         if n._op:\n",
    "#             dot.node(name=str(id(n)) + n._op, label=n._op)\n",
    "#             dot.edge(str(id(n)) + n._op, str(id(n)))\n",
    "    \n",
    "#     for n1, n2 in edges:\n",
    "#         dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "    \n",
    "#     return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d869fa7-1909-4ce3-b3e8-8c5c543b15e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_dot(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b799fe-1d58-4ede-a871-bedc66eec332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "    \n",
    "    # This is what is returned when an object of Neuron is called with some x like n(x) below\n",
    "    def __call__(self, x):\n",
    "        # sum takes an optional second argument to begin adding to instead of 0 which is the same as saying activation = sum(wi * xi for wi, xi in zip(self.w, x)) + self.b\n",
    "        activation = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        o = activation.tanh()\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049aafaa-da19-4036-a302-3d4bb81c7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [2.0, 3.0]\n",
    "n = Neuron(2)\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c96a02-9b65-4c5d-a84d-584feccbd97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        # Each i/p feature feeds into each neuron (Neuron(nin)) and we want nout such neurons in the layer\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # 1 output from each neuron in the layer\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4ee109-033c-48e8-acba-6a700a3d7d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [2.0, 3.0]\n",
    "n = Layer(2, 3)\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e54b63-fe9c-4ff1-9e36-ec931dbe74fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    # nout is a list containing the number of neurons in each layer\n",
    "    def __init__(self, nin, nout):\n",
    "        tot = [nin] + nout\n",
    "        self.layers = [Layer(tot[i], tot[i + 1]) for i in range(len(nout))]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        o = [layer(x) for layer in self.layers]\n",
    "        # The nn output is the output of the last layer\n",
    "        return o[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcbe266-f79c-4c31-a88a-c18803e6bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [2.0, 3.0, -1]\n",
    "n = MLP(3, [4, 4, 1])\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad707211-2554-4d5a-b520-219f5e7ddba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out on a tiny dataset\n",
    "xs = [\n",
    "    [2, 3, -1],\n",
    "    [3, -1, 0.5],\n",
    "    [0.5, 1, 1],\n",
    "    [1, 1, -1],\n",
    "]\n",
    "\n",
    "ys = [1, -1, -1, 1]\n",
    "\n",
    "ypred = [n(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe71eb7-cd28-4762-a549-af7a4ed8d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = sum([(ypredi - ysi)**2 for ysi, ypredi in zip(ys, ypred)])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c99683-cd69-4111-898a-66bcb46dacb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473af0c-e144-41ae-97f2-d4585ecd4afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.layers[0].neuron[0].w[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
